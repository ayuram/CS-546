{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ktgwq08Pfxk"
      },
      "outputs": [],
      "source": [
        "%pip install -qU langchain langchain-openai\n",
        "\n",
        "%pip install langchain-google-genai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "import os\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "os.environ['GOOGLE_API_KEY'] = \"api_key\"\n",
        "\n",
        "# Initialize an LLM\n",
        "llm = ChatGoogleGenerativeAI(model = \"gemini-1.5-flash-latest\",\n",
        "    temperature=0,\n",
        "    max_tokens=None,\n",
        "    timeout=None,\n",
        "    max_retries=2,)\n",
        "\n",
        "# see the list of models here : https://ai.google.dev/gemini-api/docs/models/gemini\n"
      ],
      "metadata": {
        "id": "Dy3tkjBRPscy"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "from typing import Literal, Optional, Tuple\n",
        "\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "\n",
        "\n",
        "class SubQuery(BaseModel):\n",
        "    \"\"\"Subtask of the original query which can't be broken further, and that which can be solved by an expert finetuned LLM.\"\"\"\n",
        "\n",
        "    sub_query: str = Field(\n",
        "        ...,\n",
        "        description=\"The minimal and specific subtask that can be solved by an expert finetuned LLM.\",\n",
        "    )"
      ],
      "metadata": {
        "id": "VyGOHYEuP2vT"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import PydanticToolsParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "# from langchain_openai import ChatOpenAI\n",
        "\n",
        "system = \"\"\"You are a helpful assistant that prepares queries that will be sent to a search component.\n",
        "Sometimes, these queries are very complex.\n",
        "Your job is to simplify complex queries into multiple queries that can be answered\n",
        "in isolation to eachother.\n",
        "\n",
        "If the query is simple, then keep it as it is.\n",
        "Examples\n",
        "1. Query: Did Microsoft or Google make more money last year?\n",
        "   Decomposed Questions: [Question(question='How much profit did Microsoft make last year?', answer=None), Question(question='How much profit did Google make last year?', answer=None)]\n",
        "2. Query: What is the capital of France?\n",
        "   Decomposed Questions: [Question(question='What is the capital of France?', answer=None)]\n",
        "3. Query: {{question}}\n",
        "   Decomposed Questions:\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        (\"human\", \"{question}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "llm_with_tools = llm.bind_tools([SubQuery])\n",
        "parser = PydanticToolsParser(tools=[SubQuery])\n",
        "query_analyzer = prompt | llm_with_tools | parser"
      ],
      "metadata": {
        "id": "fDLsbokEQAz0"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = 'What is the capital of France? Is it the birthplace of Napoleon? Was he ever the emperor?'\n",
        "result = query_analyzer.invoke(\n",
        "    {\n",
        "        \"question\": question\n",
        "    }\n",
        ")\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wwXtrG7VQCuR",
        "outputId": "c3e8a1e7-2e11-4e2f-b0c0-f7d84034ba36"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[SubQuery(sub_query='What is the capital of France?'),\n",
              " SubQuery(sub_query='Is the capital of France the birthplace of Napoleon?'),\n",
              " SubQuery(sub_query='Was Napoleon ever the emperor of France?')]"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Who is the current US President's wife?\"\n",
        "result = query_analyzer.invoke(\n",
        "    {\n",
        "        \"question\": question\n",
        "    }\n",
        ")\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3yzsLiEuXaZk",
        "outputId": "28fcb69c-eec9-428c-d311-6c85275c5e79"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[SubQuery(sub_query=\"Who is the current US President\\\\'s wife?\")]"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YyBhkONGYRaU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}