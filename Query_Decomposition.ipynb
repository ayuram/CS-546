{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ktgwq08Pfxk"
      },
      "outputs": [],
      "source": [
        "%pip install -qU langchain langchain-openai\n",
        "\n",
        "%pip install langchain-google-genai\n",
        "\n",
        "%pip install --upgrade --quiet llamaapi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "import os\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "os.environ['GOOGLE_API_KEY'] = \"api-key\"\n",
        "\n",
        "# Initialize an LLM\n",
        "llm = ChatGoogleGenerativeAI(model = \"gemini-1.5-pro\",\n",
        "    temperature=0,\n",
        "    max_tokens=None,\n",
        "    timeout=None,\n",
        "    max_retries=2,)\n",
        "\n",
        "# see the list of models here : https://ai.google.dev/gemini-api/docs/models/gemini\n"
      ],
      "metadata": {
        "id": "Dy3tkjBRPscy"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To use llama (no tool binding available for llama)\n",
        "# %pip install langchain-experimental\n",
        "\n",
        "# from llamaapi import LlamaAPI\n",
        "# from langchain_experimental.llms import ChatLlamaAPI\n",
        "\n",
        "# # Replace 'Your_API_Token' with your actual API token\n",
        "# llama = LlamaAPI(\"api-key\")\n",
        "\n",
        "# llm = ChatLlamaAPI(client=llama)"
      ],
      "metadata": {
        "id": "Sz3iTstb-8Ue"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "from typing import Literal, Optional, Tuple\n",
        "\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "\n",
        "\n",
        "class SubQuery(BaseModel):\n",
        "    \"\"\"Subtask of the original query which can't be broken further, and that which can be solved by an expert finetuned LLM.\"\"\"\n",
        "\n",
        "    sub_query: str = Field(\n",
        "        ...,\n",
        "        description=\"The minimal and specific subtask that can be solved by an expert finetuned LLM.\",\n",
        "    )"
      ],
      "metadata": {
        "id": "VyGOHYEuP2vT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78acce45-fc96-463f-a7a9-b8682b6da865"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py:3553: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
            "\n",
            "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
            "with: `from pydantic import BaseModel`\n",
            "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
            "\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import PydanticToolsParser\n",
        "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
        "from langchain_core.messages import AIMessage, HumanMessage, ToolMessage\n",
        "\n",
        "\n",
        "system = \"\"\"You are a helpful assistant that prepares queries that will be sent to a search component.\n",
        "Sometimes, these queries are very complex.\n",
        "Your job is to simplify complex queries into multiple queries that can be answered\n",
        "in isolation to eachother.\n",
        "\"\"\"\n",
        "# # This is a prompt template used to format each individual example.\n",
        "# example_prompt = ChatPromptTemplate.from_messages(\n",
        "#     [\n",
        "#         (\"human\", \"{question}\"),\n",
        "#         (\"ai\", \"{output}\"),\n",
        "#     ]\n",
        "# )\n",
        "\n",
        "# examples = [\n",
        "#     {\"question\": \"Who lived longer, Muhammad Ali or Alan Turing?\", \"output\": \"\"\"Subtask1: How old was Muhammad Ali when he died? Subtask2: How old was Alan Turing when he died?\"\"\"},\n",
        "#     {\"question\": \"When was the founder of craigslist born?\", \"output\": \"\"\"\n",
        "#     Subtask1: Who was the founder of craigslist?\n",
        "#     Intermediate answer: Craigslist was founded by Craig Newmark.\n",
        "#     Subtask2: When was Craig Newmark born?\n",
        "#     Intermediate answer: Craig Newmark was born on December 6, 1952\"\"\"},\n",
        "#     {\"question\": \"Who is the current US President's wife?\",\n",
        "#      \"output\": \"Subquery1 = Who is the current US president? Intermediate Answer : Donald Trump. Subquery 2 = Who is the wife of Donald Trump? Answer:Melania Trump. You will return Subquery 1 and Subquery 2 as the output.\"},\n",
        "\n",
        "# ]\n",
        "\n",
        "# few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
        "#     example_prompt=example_prompt,\n",
        "#     examples=examples,\n",
        "# )\n",
        "\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        # few_shot_prompt,\n",
        "        (\"human\", \"{question}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "llm_with_tools = llm.bind_tools([SubQuery])\n",
        "parser = PydanticToolsParser(tools=[SubQuery])\n",
        "query_analyzer = prompt | llm_with_tools | parser"
      ],
      "metadata": {
        "id": "fDLsbokEQAz0"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = 'What is the capital of France? Is it the birthplace of Napoleon? Was he ever the emperor?'\n",
        "result = query_analyzer.invoke(\n",
        "    {\n",
        "        \"question\": question\n",
        "    }\n",
        ")\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wwXtrG7VQCuR",
        "outputId": "f0d0181f-25ca-40be-c600-aa5395e00f1f"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[SubQuery(sub_query='What is the capital of France?'),\n",
              " SubQuery(sub_query='Is Paris the birthplace of Napoleon?'),\n",
              " SubQuery(sub_query='Was Napoleon ever the emperor?')]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Who lived longer, Muhammad Ali or Alan Turing?\"\n",
        "result = query_analyzer.invoke(\n",
        "    {\n",
        "        \"question\": question\n",
        "    }\n",
        ")\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3yzsLiEuXaZk",
        "outputId": "a285c02c-6e09-4ccc-a2fe-5e5c337489a3"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[SubQuery(sub_query='How long did Muhammad Ali live?'),\n",
              " SubQuery(sub_query='How long did Alan Turing live?')]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"When was the founder of craigslist born?\"\n",
        "result = query_analyzer.invoke(\n",
        "    {\n",
        "        \"question\": question\n",
        "    }\n",
        ")\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26lJxhrIGF4Y",
        "outputId": "e9357cfc-da20-4f24-8db1-a8825f82b62b"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[SubQuery(sub_query='Who is the founder of craigslist?'),\n",
              " SubQuery(sub_query='When was Craig Newmark born?')]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What is the formula for Einsten's most famous equation and is it related to gravity?\"\n",
        "result = query_analyzer.invoke(\n",
        "    {\n",
        "        \"question\": question\n",
        "    }\n",
        ")\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDsd-jtkHMz-",
        "outputId": "7dd96c9f-efe5-4b54-9187-b5c7c2f0e37f"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[SubQuery(sub_query=\"What is the formula for Einstein\\\\'s most famous equation?\"),\n",
              " SubQuery(sub_query=\"Is Einstein\\\\'s most famous equation related to gravity?\")]"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hc_HMoXiIBos"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}